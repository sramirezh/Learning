{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('simon3': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cacdde318d08a46d07035f2dd41348c7e0164a68186675573f13e987da9ac40f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Date').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Spark_DataFrames/appl_stock.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+\n|      Date|              Open|\n+----------+------------------+\n|2010-01-04|        213.429998|\n|2010-01-05|        214.599998|\n|2010-01-06|        214.379993|\n|2010-01-07|            211.75|\n|2010-01-08|        210.299994|\n|2010-01-11|212.79999700000002|\n|2010-01-12|209.18999499999998|\n|2010-01-13|        207.870005|\n|2010-01-14|210.11000299999998|\n|2010-01-15|210.92999500000002|\n|2010-01-19|        208.330002|\n|2010-01-20|        214.910006|\n|2010-01-21|        212.079994|\n|2010-01-22|206.78000600000001|\n|2010-01-25|202.51000200000001|\n|2010-01-26|205.95000100000001|\n|2010-01-27|        206.849995|\n|2010-01-28|        204.930004|\n|2010-01-29|        201.079996|\n|2010-02-01|192.36999699999998|\n+----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(['Date', 'Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month, year, weekofyear, format_number, date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+\n|dayofmonth(Date)|\n+----------------+\n|               4|\n|               5|\n|               6|\n|               7|\n|               8|\n|              11|\n|              12|\n|              13|\n|              14|\n|              15|\n|              19|\n|              20|\n|              21|\n|              22|\n|              25|\n|              26|\n|              27|\n|              28|\n|              29|\n|               1|\n+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofmonth(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n|year(Date)|\n+----------+\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n|      2010|\n+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# We want the average closing price per year\n",
    "df.select(year(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+------------------+\n|Year|        avg(Close)|\n+----+------------------+\n|2015|120.03999980555547|\n|2013| 472.6348802857143|\n|2014| 295.4023416507935|\n|2012| 576.0497195640002|\n|2016|104.60400786904763|\n|2010| 259.8424600000002|\n|2011|364.00432532142867|\n+----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# We want the average closing price per year\n",
    "newdf = df.withColumn(\"Year\", year(df['Date'])) # Creating new column\n",
    "newdf.groupBy(\"Year\").mean().select(['Year', 'avg(Close)']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = newdf.groupBy(\"Year\").mean().select(['Year', 'avg(Close)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = result.withColumnRenamed(\"avg(Close)\", \"Average Closing price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---------+\n|Year|avg close|\n+----+---------+\n|2015|   120.04|\n|2013|   472.63|\n|2014|   295.40|\n|2012|   576.05|\n|2016|   104.60|\n|2010|   259.84|\n|2011|   364.00|\n+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "new.select(['Year', format_number('Average Closing price', 2).alias(\"avg close\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}